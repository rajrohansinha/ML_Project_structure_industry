-------------------------------### ğŸŒ **1ï¸âƒ£ **Open Anaconda Prompt**

2ï¸âƒ£ **Navigate to your project folder**
Use `cd` to move to your project location:

```bash
cd C:\Users\rroha\OneDrive\Desktop\Raj\ML_Project_Industrial
```

3ï¸âƒ£ **Create a virtual environment**

```bash
conda create -p venv python==3.8 -y
```

4ï¸âƒ£ **Activate the environment**

```bash
conda activate venv/
```

----------------------------------### ğŸŒ **2ï¸âƒ£ Setup GitHub Repository (VS Code Terminal)**

1ï¸âƒ£ Initialize Git

git init

2ï¸âƒ£ Set main branch to `main`

git branch -M main

3ï¸âƒ£ Link local project to GitHub

git remote add origin https://github.com/rajrohansinha/ML_Project_structure_industry.git

4ï¸âƒ£ Verify remote link

git remote -v

5ï¸âƒ£ Push your code to GitHub

git push -u origin main

6ï¸âƒ£ Pull (sync) from GitHub

git pull

----------------------------------### ğŸ— **3ï¸âƒ£ Create Project Skeleton**

ğŸ“‚ **Key files and folders created:**

* `README.md` â†’ Project description & setup guide.
* `requirements.txt` â†’ List of Python libraries needed.
* `setup.py` â†’ Packaging instructions for the project.
* `.gitignore` â†’ Ignores unnecessary files (e.g., `venv/`, `__pycache__/`).
* `src/` â†’ Main source code folder.

  * `__init__.py` â†’ Marks folder as a Python package.

----------------------------------### ğŸ“¦ **4ï¸âƒ£ Setup `setup.py` for packaging**

This allows your project to be installed as a package.

```python
from setuptools import find_packages, setup
from typing import List

HYPEN_E_DOT='-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements=file_obj.readlines()
        requirements=[req.replace("\n","") for req in requirements]

        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    return requirements

setup(
    name='ml_project_industrial',
    version='0.0.1',
    author='Rohan',
    author_email='r.rohan921998@gmail.com',
    packages=find_packages(),
    install_requires=get_requirements('requirements.txt')
)


----------------------------------### ğŸ“œ **5ï¸âƒ£ Add `requirements.txt`**

* Add all libraries your project needs (like pandas, numpy, scikit-learn).
* Install with:

pip install -r requirements.txt

----------------------------------### ğŸš« **6ï¸âƒ£ Add `.gitignore`**

Keeps the repo clean by ignoring files that donâ€™t need to be tracked.

# Virtual environment
venv/

# Python cache
__pycache__/
*.pyc
*.pyo

# VS Code settings
.vscode/

# System files
.DS_Store
Thumbs.db

----------------------------------### ğŸ”„ **7ï¸âƒ£ Stage, Commit & Push**

* `git status` â†’ Check changes
* `git add .` â†’ Stage all files
* `git commit -m "setup"` â†’ Save your work
* `git push -u origin main` â†’ Upload to GitHub

âœ… **Stage 1 â€“ Code Added Inside `src/`**

ğŸ“‚ Inside **src/:**

* âœ… `__init__.py` â†’ Marks `src` as a package.
* âœ… `logger.py` â†’ Logging setup.

  * Creates `logs/` folder automatically.
  * Generates timestamped log files.

import logging, os
from datetime import datetime
LOG_FILE = f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"
logs_path = os.path.join(os.getcwd(), "logs")
os.makedirs(logs_path, exist_ok=True)
LOG_FILE_PATH = os.path.join(logs_path, LOG_FILE)
logging.basicConfig(
    filename=LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
if __name__ == "__main__":
    logging.info("Logging has started successfully.")

To run the above code : python src/logger.py

âœ” A new log file is created inside `logs/`.

âœ… `exception.py` â†’ Custom exception handler.

  * Shows **file name, line number, and error message**.

import sys
from src.logger import logging

def error_message_detail(error, error_detail: sys):
    _, _, exc_tb = error_detail.exc_info()
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in python script name [{0}] line number [{1}] error message[{2}]".format(
        file_name, exc_tb.tb_lineno, str(error)
    )
    return error_message

class CustomException(Exception):
    def __init__(self, error_message, error_detail: sys):
        super().__init__(error_message)
        self.error_message = error_message_detail(error_message, error_detail=error_detail)
    
    def __str__(self):
        return self.error_message

* âœ… `utils.py` 

ğŸ“‚ **components/**

* `__init__.py
* `data_ingestion.py
* `data_transformation.py
* `model_trainer.py

ğŸ“‚ **pipeline/**

* __init__.py
* train_pipeline.py
-----------------------------------------------------------------------------------------------------------------------------
ML_Project_Industrial/
â”‚
â”œâ”€â”€ venv/                          # Virtual environment (ignored by Git)
â”œâ”€â”€ logs/                          # Auto-created folder for logs
â”‚   â””â”€â”€ 07_31_2025_12_55_26.log    # Example log file
â”‚
â”œâ”€â”€ src/                           # Main source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ exception.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ train_pipeline.py
â”‚
â”œâ”€â”€ Steps to follow to setup ML code
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
---------------------------------------------------------------------------------------------------
<<------------data_ingestion.py---------->>

Within my ML_Project_Industrial folder, I created a structured pipeline under the src directory. Inside src/components/, 
I wrote the data_ingestion.py file using the code provided. This script was executed using the command:

python -m src.components.data_ingestion
Running this successfully:

âœ… Created an artifacts/ folder automatically
âœ… Saved three files inside: data.csv (raw data), train.csv (80% training data), and test.csv (20% testing data)
âœ… Generated a logs/ folder where all pipeline logs are stored (useful for debugging & tracking runs)

# â€¢ JUPYTER NOTEBOOKS are great for:
  - Quick experiments, EDA, and visualizations
   - Fast tweaking of code and sharing insights
   - BUT they are not ideal for production, automation, or clean structure

# â€¢ PYTHON SCRIPTS & PIPELINES (like data_ingestion.py):
   - Make the project modular & reusable
   - Run with one command: python -m src.components.data_ingestion
   - Auto-create folders (artifacts/, logs/) to store outputs & track runs
   - Easier to version control (Git), deploy, and automate (CI/CD)
   - Form the foundation for adding steps (data_transformation.py, model_trainer.py)

# â€¢ WHY THIS MATTERS:
   - Moves the project from â€œnotebook experimentsâ€ to a clean, production-ready structure
   - Supports scaling, automation, and collaboration

Jupyter notebooks = for exploration & learning
Python modules like data_ingestion.py = for scaling, automation, and production readiness
This shift means Iâ€™m not just â€œrunning ML experiments,â€ Iâ€™m building a real ML system that could run in an actual industrial setting.   