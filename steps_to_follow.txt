-------------------------------### 🌐 **1️⃣ **Open Anaconda Prompt**

2️⃣ **Navigate to your project folder**
Use `cd` to move to your project location:

```bash
cd C:\Users\rroha\OneDrive\Desktop\Raj\ML_Project_Industrial
```

3️⃣ **Create a virtual environment**

```bash
conda create -p venv python==3.8 -y
```

4️⃣ **Activate the environment**

```bash
conda activate venv/
```

----------------------------------### 🌐 **2️⃣ Setup GitHub Repository (VS Code Terminal)**

1️⃣ Initialize Git

git init

2️⃣ Set main branch to `main`

git branch -M main

3️⃣ Link local project to GitHub

git remote add origin https://github.com/rajrohansinha/ML_Project_structure_industry.git

4️⃣ Verify remote link

git remote -v

5️⃣ Push your code to GitHub

git push -u origin main

6️⃣ Pull (sync) from GitHub

git pull

----------------------------------### 🏗 **3️⃣ Create Project Skeleton**

📂 **Key files and folders created:**

* `README.md` → Project description & setup guide.
* `requirements.txt` → List of Python libraries needed.
* `setup.py` → Packaging instructions for the project.
* `.gitignore` → Ignores unnecessary files (e.g., `venv/`, `__pycache__/`).
* `src/` → Main source code folder.

  * `__init__.py` → Marks folder as a Python package.

----------------------------------### 📦 **4️⃣ Setup `setup.py` for packaging**

This allows your project to be installed as a package.

```python
from setuptools import find_packages, setup
from typing import List

HYPEN_E_DOT='-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements=file_obj.readlines()
        requirements=[req.replace("\n","") for req in requirements]

        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    return requirements

setup(
    name='ml_project_industrial',
    version='0.0.1',
    author='Rohan',
    author_email='r.rohan921998@gmail.com',
    packages=find_packages(),
    install_requires=get_requirements('requirements.txt')
)


----------------------------------### 📜 **5️⃣ Add `requirements.txt`**

* Add all libraries your project needs (like pandas, numpy, scikit-learn).
* Install with:

pip install -r requirements.txt

----------------------------------### 🚫 **6️⃣ Add `.gitignore`**

Keeps the repo clean by ignoring files that don’t need to be tracked.

# Virtual environment
venv/

# Python cache
__pycache__/
*.pyc
*.pyo

# VS Code settings
.vscode/

# System files
.DS_Store
Thumbs.db

----------------------------------### 🔄 **7️⃣ Stage, Commit & Push**

* `git status` → Check changes
* `git add .` → Stage all files
* `git commit -m "setup"` → Save your work
* `git push -u origin main` → Upload to GitHub

✅ **Stage 1 – Code Added Inside `src/`**

📂 Inside **src/:**

* ✅ `__init__.py` → Marks `src` as a package.
* ✅ `logger.py` → Logging setup.

  * Creates `logs/` folder automatically.
  * Generates timestamped log files.

import logging, os
from datetime import datetime
LOG_FILE = f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"
logs_path = os.path.join(os.getcwd(), "logs")
os.makedirs(logs_path, exist_ok=True)
LOG_FILE_PATH = os.path.join(logs_path, LOG_FILE)
logging.basicConfig(
    filename=LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
if __name__ == "__main__":
    logging.info("Logging has started successfully.")

To run the above code : python src/logger.py

✔ A new log file is created inside `logs/`.

✅ `exception.py` → Custom exception handler.

  * Shows **file name, line number, and error message**.

import sys
from src.logger import logging

def error_message_detail(error, error_detail: sys):
    _, _, exc_tb = error_detail.exc_info()
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in python script name [{0}] line number [{1}] error message[{2}]".format(
        file_name, exc_tb.tb_lineno, str(error)
    )
    return error_message

class CustomException(Exception):
    def __init__(self, error_message, error_detail: sys):
        super().__init__(error_message)
        self.error_message = error_message_detail(error_message, error_detail=error_detail)
    
    def __str__(self):
        return self.error_message

* ✅ `utils.py` 

📂 **components/**

* `__init__.py
* `data_ingestion.py
* `data_transformation.py
* `model_trainer.py

📂 **pipeline/**

<<------------data_ingestion.py---------->>

Within my ML_Project_Industrial folder, I created a structured pipeline under the src directory. Inside src/components/, 
I wrote the data_ingestion.py file using the code provided. This script was executed using the command:

>>>>>>>>>>>>>>>>>>>>>>>>>>> python -m src.components.data_ingestion
Running this successfully:

✅ Created an artifacts/ folder automatically
✅ Saved three files inside: data.csv (raw data), train.csv (80% training data), and test.csv (20% testing data)
✅ Generated a logs/ folder where all pipeline logs are stored (useful for debugging & tracking runs)

# • JUPYTER NOTEBOOKS are great for:
  - Quick experiments, EDA, and visualizations
   - Fast tweaking of code and sharing insights
   - BUT they are not ideal for production, automation, or clean structure

# • PYTHON SCRIPTS & PIPELINES (like data_ingestion.py):
   - Make the project modular & reusable
   - Run with one command: python -m src.components.data_ingestion
   - Auto-create folders (artifacts/, logs/) to store outputs & track runs
   - Easier to version control (Git), deploy, and automate (CI/CD)
   - Form the foundation for adding steps (data_transformation.py, model_trainer.py)

# • WHY THIS MATTERS:
   - Moves the project from “notebook experiments” to a clean, production-ready structure
   - Supports scaling, automation, and collaboration

Jupyter notebooks = for exploration & learning
Python modules like data_ingestion.py = for scaling, automation, and production readiness
This shift means I’m not just “running ML experiments,” I’m building a real ML system that could run in an actual industrial setting.  

<<------------data_transformation.py---------->>

✅ Read the train.csv & test.csv files created earlier by data_ingestion.py
✅ Built a preprocessing pipeline using Scikit-learn Pipelines & ColumnTransformer
    • Numerical features → Missing values filled with median, scaled using StandardScaler
    • Categorical features → Missing values filled with most frequent, one-hot encoded, scaled
✅ Saved the preprocessing object as preprocessor.pkl inside the artifacts/ folder (so we can reuse it later in training or deployment)
✅ Returned transformed NumPy arrays for train & test data (ready for feeding into ML models)

📌 Why is this step important?
✅ Ensures data is cleaned, encoded & standardized before training (consistent for all future data)
✅ Preprocessor object (preprocessor.pkl) means we don’t need to repeat all steps manually again — load it anywhere (model training, deployment)

>>>>>>>>>>>>>>>>>>>>>>>>>>> python -m src.components.data_transformation

<<------------model_trainer.py---------->>

✅ Read transformed train & test NumPy arrays from data_transformation.py
✅ Split arrays into features (X) and target (y) for training & testing
✅ Defined a dictionary of ML models to evaluate:
    • RandomForestRegressor
    • DecisionTreeRegressor
    • GradientBoostingRegressor
    • LinearRegression
    • XGBRegressor (XGBoost)
    • CatBoostRegressor (CatBoost)
    • AdaBoostRegressor
✅ Created a hyperparameter grid for each model (where applicable)
✅ Called evaluate_models() to:
    • Run GridSearchCV on all models
    • Find best hyperparameters
    • Return performance report (R² scores)
✅ Selected the BEST model based on R² score
✅ Checked if best model is “good enough” (R² ≥ 0.6) — else raised exception
✅ Saved the trained best model as model.pkl inside artifacts/ for future use
✅ Returned the final R² score (how well the model predicts on unseen data)

📌 Why is this step important?
✅ Automates the process of testing multiple ML algorithms with hyperparameter tuning
✅ Ensures only the **best-performing model** is saved (model.pkl) for deployment
✅ Maintains a reusable pipeline — you can load model.pkl later for predictions without retraining

>>>>>>>>>>>>>>>>>>>>>>>>>>> python -m src.components.model_trainer
Running this successfully:

<<------------model_trainer.py RUN OUTPUT SUMMARY---------->>

✅ Data Ingestion:
    • Successfully loaded dataset from notebook/data/stud.csv (1000 rows, 8 columns)
    • Saved raw dataset to artifacts/data.csv
    • Performed train-test split → Train: (800, 8), Test: (200, 8)
    • Saved train.csv & test.csv into artifacts/

✅ Data Transformation:
    • Loaded train.csv & test.csv
    • Identified columns:
        - Categorical: gender, race_ethnicity, parental_level_of_education, lunch, test_preparation_course
        - Numerical: writing_score, reading_score
    • Applied preprocessing pipeline (imputation, encoding, scaling)
    • Saved preprocessing object as artifacts/preprocessor.pkl
    • Returned transformed arrays → Train: (800, 20), Test: (200, 20)

✅ Model Training:
    • Split transformed arrays into X_train, y_train, X_test, y_test
    • Evaluated multiple models with GridSearchCV:
        - RandomForestRegressor
        - DecisionTreeRegressor
        - GradientBoostingRegressor
        - LinearRegression
        - XGBRegressor
        - CatBoostRegressor
        - AdaBoostRegressor
    • Compared R² scores across models

🏆 Best Model Selected:
    • **Linear Regression**
    • **R² Score: 0.8775** (Excellent performance)

💾 Model Saving:
    • Saved final trained model as artifacts/model.pkl

🎯 **Final Result:**
✅ End-to-end ML pipeline ran successfully (Ingestion → Transformation → Training)
✅ Best model (Linear Regression) stored for deployment
✅ Final R² Score: **0.8775**

-----------------------------------------predict\_pipeline.py-------------------------------------------------------------------------

Why first?** Because your Flask app will depend on it to predict. If you don’t have this ready, the app can’t “think.”
Create `src/pipeline/predict_pipeline.py`.
Add:
    * `CustomData` → converts HTML form inputs into a Pandas DataFrame.
    * `PredictPipeline` → loads `model.pkl` & `preprocessor.pkl` from `artifacts/`, transforms the data, and predicts.
You now have an ML “engine room” ready.

-------------------------Build the Flask App File (Prediction\_application.py)----------------------------------------------------------

Once the ML engine is ready, you can wire up Flask to serve predictions.

Create `Prediction_application.py`.
Add:
    * `index()` route → Shows `index.html`.
    * `predict_datapoint()` route → Shows `home.html` (GET) & handles prediction (POST).
Your backend (Flask server) is set up but won’t show anything until HTML templates exist.

-----------------------------------Create the Templates (HTML files)------------------------------------------------------

Make a folder named `templates/`.
Inside it:
    * `index.html` → Simple welcome page + “Go to Prediction” button.
    * `home.html` → Beautiful form with **dropdowns** (gender, lunch, etc.), number boxes (scores), and **Predict** button.
Your web app has a front‑end and will display predictions.

Run Flask app: python Prediction_application.py
Open in browser: `http://127.0.0.1:5000/`

------------------------GIT STEPS AFTER WRITING A NEW PIPELINE (e.g., model_trainer.py)-----------------------------------------

1️⃣ Stage all changes (new file, edits, etc.) : git add .
2️⃣ Commit the changes with a clear message : git commit -m "---------"
3️⃣ (Optional) Check status to see what’s pending / ahead : git status    
4️⃣ Push changes to GitHub (first push uses -u to set upstream) : git push -u origin main

------------------------------Code structure-------------------------------------------------------------------------------
ML_Project_Industrial/
│
├── venv/                          # ✅ Virtual environment (ignored by Git)
│
├── logs/                          # ✅ Folder for logs
│   └── 07_31_2025_12_55_26.log    # Example log file
│
├── artifacts/                     # ✅ Stores trained model + preprocessor
│   ├── model.pkl
│   └── preprocessor.pkl
│
├── src/                           # ✅ Core ML source code
│   ├── __init__.py
│   ├── logger.py
│   ├── exception.py
│   ├── utils.py
│   │
│   ├── components/                # ✅ Handles ML pipeline components
│   │   ├── __init__.py
│   │   ├── data_ingestion.py      # Reads raw data
│   │   ├── data_transformation.py # Prepares data for training
│   │   ├── model_trainer.py       # Trains & saves model.pkl + preprocessor.pkl
│   │
│   └── pipeline/                  
│       ├── __init__.py
│       ├── train_pipeline.py      # Runs end-to-end training
│       └── predict_pipeline.py    # ✅ NEW → Converts form data into DF & predicts
│
├── templates/                     # ✅ NEW → Flask HTML Templates Folder
│   ├── index.html                 # Homepage (Welcome page)
│   └── home.html                  # Prediction form with dropdowns & input boxes
│
├── static/                        # (Optional for CSS/JS/Images if needed later)
│
├── Prediction_application.py      # ✅ NEW → Flask app entry point
│
├── Steps to follow to setup ML code   # ✅ Your step-by-step setup notes
├── requirements.txt               # Python dependencies (Flask, sklearn, etc.)
├── setup.py                       # Package setup if needed for pip install
├── .gitignore                     # Ignore venv, artifacts, logs, etc.
└── README.md                      # High-level project explanation