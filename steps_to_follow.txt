-------------------------------### ğŸŒ **1ï¸âƒ£ **Open Anaconda Prompt**

2ï¸âƒ£ **Navigate to your project folder**
Use `cd` to move to your project location:

```bash
cd C:\Users\rroha\OneDrive\Desktop\Raj\ML_Project_Industrial
```

3ï¸âƒ£ **Create a virtual environment**

```bash
conda create -p venv python==3.8 -y
```

4ï¸âƒ£ **Activate the environment**

```bash
conda activate venv/
```

----------------------------------### ğŸŒ **2ï¸âƒ£ Setup GitHub Repository (VS Code Terminal)**

1ï¸âƒ£ Initialize Git

git init

2ï¸âƒ£ Set main branch to `main`

git branch -M main

3ï¸âƒ£ Link local project to GitHub

git remote add origin https://github.com/rajrohansinha/ML_Project_structure_industry.git

4ï¸âƒ£ Verify remote link

git remote -v

5ï¸âƒ£ Push your code to GitHub

git push -u origin main

6ï¸âƒ£ Pull (sync) from GitHub

git pull

----------------------------------### ğŸ— **3ï¸âƒ£ Create Project Skeleton**

ğŸ“‚ **Key files and folders created:**

* `README.md` â†’ Project description & setup guide.
* `requirements.txt` â†’ List of Python libraries needed.
* `setup.py` â†’ Packaging instructions for the project.
* `.gitignore` â†’ Ignores unnecessary files (e.g., `venv/`, `__pycache__/`).
* `src/` â†’ Main source code folder.

  * `__init__.py` â†’ Marks folder as a Python package.

----------------------------------### ğŸ“¦ **4ï¸âƒ£ Setup `setup.py` for packaging**

This allows your project to be installed as a package.

```python
from setuptools import find_packages, setup
from typing import List

HYPEN_E_DOT='-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements=file_obj.readlines()
        requirements=[req.replace("\n","") for req in requirements]

        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    return requirements

setup(
    name='ml_project_industrial',
    version='0.0.1',
    author='Rohan',
    author_email='r.rohan921998@gmail.com',
    packages=find_packages(),
    install_requires=get_requirements('requirements.txt')
)


----------------------------------### ğŸ“œ **5ï¸âƒ£ Add `requirements.txt`**

* Add all libraries your project needs (like pandas, numpy, scikit-learn).
* Install with:

pip install -r requirements.txt

----------------------------------### ğŸš« **6ï¸âƒ£ Add `.gitignore`**

Keeps the repo clean by ignoring files that donâ€™t need to be tracked.

# Virtual environment
venv/

# Python cache
__pycache__/
*.pyc
*.pyo

# VS Code settings
.vscode/

# System files
.DS_Store
Thumbs.db

----------------------------------### ğŸ”„ **7ï¸âƒ£ Stage, Commit & Push**

* `git status` â†’ Check changes
* `git add .` â†’ Stage all files
* `git commit -m "setup"` â†’ Save your work
* `git push -u origin main` â†’ Upload to GitHub

âœ… **Stage 1 â€“ Code Added Inside `src/`**

ğŸ“‚ Inside **src/:**

* âœ… `__init__.py` â†’ Marks `src` as a package.
* âœ… `logger.py` â†’ Logging setup.

  * Creates `logs/` folder automatically.
  * Generates timestamped log files.

import logging, os
from datetime import datetime
LOG_FILE = f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"
logs_path = os.path.join(os.getcwd(), "logs")
os.makedirs(logs_path, exist_ok=True)
LOG_FILE_PATH = os.path.join(logs_path, LOG_FILE)
logging.basicConfig(
    filename=LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
if __name__ == "__main__":
    logging.info("Logging has started successfully.")

To run the above code : python src/logger.py

âœ” A new log file is created inside `logs/`.

âœ… `exception.py` â†’ Custom exception handler.

  * Shows **file name, line number, and error message**.

import sys
from src.logger import logging

def error_message_detail(error, error_detail: sys):
    _, _, exc_tb = error_detail.exc_info()
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in python script name [{0}] line number [{1}] error message[{2}]".format(
        file_name, exc_tb.tb_lineno, str(error)
    )
    return error_message

class CustomException(Exception):
    def __init__(self, error_message, error_detail: sys):
        super().__init__(error_message)
        self.error_message = error_message_detail(error_message, error_detail=error_detail)
    
    def __str__(self):
        return self.error_message

* âœ… `utils.py` 

ğŸ“‚ **components/**

* `__init__.py
* `data_ingestion.py
* `data_transformation.py
* `model_trainer.py

ğŸ“‚ **pipeline/**

<<------------data_ingestion.py---------->>

Within my ML_Project_Industrial folder, I created a structured pipeline under the src directory. Inside src/components/, 
I wrote the data_ingestion.py file using the code provided. This script was executed using the command:

>>>>>>>>>>>>>>>>>>>>>>>>>>> python -m src.components.data_ingestion
Running this successfully:

âœ… Created an artifacts/ folder automatically
âœ… Saved three files inside: data.csv (raw data), train.csv (80% training data), and test.csv (20% testing data)
âœ… Generated a logs/ folder where all pipeline logs are stored (useful for debugging & tracking runs)

# â€¢ JUPYTER NOTEBOOKS are great for:
  - Quick experiments, EDA, and visualizations
   - Fast tweaking of code and sharing insights
   - BUT they are not ideal for production, automation, or clean structure

# â€¢ PYTHON SCRIPTS & PIPELINES (like data_ingestion.py):
   - Make the project modular & reusable
   - Run with one command: python -m src.components.data_ingestion
   - Auto-create folders (artifacts/, logs/) to store outputs & track runs
   - Easier to version control (Git), deploy, and automate (CI/CD)
   - Form the foundation for adding steps (data_transformation.py, model_trainer.py)

# â€¢ WHY THIS MATTERS:
   - Moves the project from â€œnotebook experimentsâ€ to a clean, production-ready structure
   - Supports scaling, automation, and collaboration

Jupyter notebooks = for exploration & learning
Python modules like data_ingestion.py = for scaling, automation, and production readiness
This shift means Iâ€™m not just â€œrunning ML experiments,â€ Iâ€™m building a real ML system that could run in an actual industrial setting.  

<<------------data_transformation.py---------->>

âœ… Read the train.csv & test.csv files created earlier by data_ingestion.py
âœ… Built a preprocessing pipeline using Scikit-learn Pipelines & ColumnTransformer
Â Â Â Â â€¢ Numerical features â†’ Missing values filled with median, scaled using StandardScaler
Â Â Â Â â€¢ Categorical features â†’ Missing values filled with most frequent, one-hot encoded, scaled
âœ… Saved the preprocessing object as preprocessor.pkl inside the artifacts/ folder (so we can reuse it later in training or deployment)
âœ… Returned transformed NumPy arrays for train & test data (ready for feeding into ML models)

ğŸ“Œ Why is this step important?
âœ… Ensures data is cleaned, encoded & standardized before training (consistent for all future data)
âœ… Preprocessor object (preprocessor.pkl) means we donâ€™t need to repeat all steps manually again â€” load it anywhere (model training, deployment)

>>>>>>>>>>>>>>>>>>>>>>>>>>> python -m src.components.data_transformation

<<------------model_trainer.py---------->>

âœ… Read transformed train & test NumPy arrays from data_transformation.py
âœ… Split arrays into features (X) and target (y) for training & testing
âœ… Defined a dictionary of ML models to evaluate:
    â€¢ RandomForestRegressor
    â€¢ DecisionTreeRegressor
    â€¢ GradientBoostingRegressor
    â€¢ LinearRegression
    â€¢ XGBRegressor (XGBoost)
    â€¢ CatBoostRegressor (CatBoost)
    â€¢ AdaBoostRegressor
âœ… Created a hyperparameter grid for each model (where applicable)
âœ… Called evaluate_models() to:
    â€¢ Run GridSearchCV on all models
    â€¢ Find best hyperparameters
    â€¢ Return performance report (RÂ² scores)
âœ… Selected the BEST model based on RÂ² score
âœ… Checked if best model is â€œgood enoughâ€ (RÂ² â‰¥ 0.6) â€” else raised exception
âœ… Saved the trained best model as model.pkl inside artifacts/ for future use
âœ… Returned the final RÂ² score (how well the model predicts on unseen data)

ğŸ“Œ Why is this step important?
âœ… Automates the process of testing multiple ML algorithms with hyperparameter tuning
âœ… Ensures only the **best-performing model** is saved (model.pkl) for deployment
âœ… Maintains a reusable pipeline â€” you can load model.pkl later for predictions without retraining

>>>>>>>>>>>>>>>>>>>>>>>>>>> python -m src.components.model_trainer
Running this successfully:

<<------------model_trainer.py RUN OUTPUT SUMMARY---------->>

âœ… Data Ingestion:
    â€¢ Successfully loaded dataset from notebook/data/stud.csv (1000 rows, 8 columns)
    â€¢ Saved raw dataset to artifacts/data.csv
    â€¢ Performed train-test split â†’ Train: (800, 8), Test: (200, 8)
    â€¢ Saved train.csv & test.csv into artifacts/

âœ… Data Transformation:
    â€¢ Loaded train.csv & test.csv
    â€¢ Identified columns:
        - Categorical: gender, race_ethnicity, parental_level_of_education, lunch, test_preparation_course
        - Numerical: writing_score, reading_score
    â€¢ Applied preprocessing pipeline (imputation, encoding, scaling)
    â€¢ Saved preprocessing object as artifacts/preprocessor.pkl
    â€¢ Returned transformed arrays â†’ Train: (800, 20), Test: (200, 20)

âœ… Model Training:
    â€¢ Split transformed arrays into X_train, y_train, X_test, y_test
    â€¢ Evaluated multiple models with GridSearchCV:
        - RandomForestRegressor
        - DecisionTreeRegressor
        - GradientBoostingRegressor
        - LinearRegression
        - XGBRegressor
        - CatBoostRegressor
        - AdaBoostRegressor
    â€¢ Compared RÂ² scores across models

ğŸ† Best Model Selected:
    â€¢ **Linear Regression**
    â€¢ **RÂ² Score: 0.8775** (Excellent performance)

ğŸ’¾ Model Saving:
    â€¢ Saved final trained model as artifacts/model.pkl

ğŸ¯ **Final Result:**
âœ… End-to-end ML pipeline ran successfully (Ingestion â†’ Transformation â†’ Training)
âœ… Best model (Linear Regression) stored for deployment
âœ… Final RÂ² Score: **0.8775**

-----------------------------------------predict\_pipeline.py-------------------------------------------------------------------------

Why first?** Because your Flask app will depend on it to predict. If you donâ€™t have this ready, the app canâ€™t â€œthink.â€
Create `src/pipeline/predict_pipeline.py`.
Add:
    * `CustomData` â†’ converts HTML form inputs into a Pandas DataFrame.
    * `PredictPipeline` â†’ loads `model.pkl` & `preprocessor.pkl` from `artifacts/`, transforms the data, and predicts.
You now have an ML â€œengine roomâ€ ready.

-------------------------Build the Flask App File (Prediction\_application.py)----------------------------------------------------------

Once the ML engine is ready, you can wire up Flask to serve predictions.

Create `Prediction_application.py`.
Add:
    * `index()` route â†’ Shows `index.html`.
    * `predict_datapoint()` route â†’ Shows `home.html` (GET) & handles prediction (POST).
Your backend (Flask server) is set up but wonâ€™t show anything until HTML templates exist.

-----------------------------------Create the Templates (HTML files)------------------------------------------------------

Make a folder named `templates/`.
Inside it:
    * `index.html` â†’ Simple welcome page + â€œGo to Predictionâ€ button.
    * `home.html` â†’ Beautiful form with **dropdowns** (gender, lunch, etc.), number boxes (scores), and **Predict** button.
Your web app has a frontâ€‘end and will display predictions.

Run Flask app: python Prediction_application.py
Open in browser: `http://127.0.0.1:5000/`

------------------------GIT STEPS AFTER WRITING A NEW PIPELINE (e.g., model_trainer.py)-----------------------------------------

1ï¸âƒ£ Stage all changes (new file, edits, etc.) : git add .
2ï¸âƒ£ Commit the changes with a clear message : git commit -m "---------"
3ï¸âƒ£ (Optional) Check status to see whatâ€™s pending / ahead : git status    
4ï¸âƒ£ Push changes to GitHub (first push uses -u to set upstream) : git push -u origin main

------------------------------Code structure-------------------------------------------------------------------------------
ML_Project_Industrial/
â”‚
â”œâ”€â”€ venv/                          # âœ… Virtual environment (ignored by Git)
â”‚
â”œâ”€â”€ logs/                          # âœ… Folder for logs
â”‚   â””â”€â”€ 07_31_2025_12_55_26.log    # Example log file
â”‚
â”œâ”€â”€ artifacts/                     # âœ… Stores trained model + preprocessor
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ preprocessor.pkl
â”‚
â”œâ”€â”€ src/                           # âœ… Core ML source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ exception.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ components/                # âœ… Handles ML pipeline components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py      # Reads raw data
â”‚   â”‚   â”œâ”€â”€ data_transformation.py # Prepares data for training
â”‚   â”‚   â”œâ”€â”€ model_trainer.py       # Trains & saves model.pkl + preprocessor.pkl
â”‚   â”‚
â”‚   â””â”€â”€ pipeline/                  
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ train_pipeline.py      # Runs end-to-end training
â”‚       â””â”€â”€ predict_pipeline.py    # âœ… NEW â†’ Converts form data into DF & predicts
â”‚
â”œâ”€â”€ templates/                     # âœ… NEW â†’ Flask HTML Templates Folder
â”‚   â”œâ”€â”€ index.html                 # Homepage (Welcome page)
â”‚   â””â”€â”€ home.html                  # Prediction form with dropdowns & input boxes
â”‚
â”œâ”€â”€ static/                        # (Optional for CSS/JS/Images if needed later)
â”‚
â”œâ”€â”€ Prediction_application.py      # âœ… NEW â†’ Flask app entry point
â”‚
â”œâ”€â”€ Steps to follow to setup ML code   # âœ… Your step-by-step setup notes
â”œâ”€â”€ requirements.txt               # Python dependencies (Flask, sklearn, etc.)
â”œâ”€â”€ setup.py                       # Package setup if needed for pip install
â”œâ”€â”€ .gitignore                     # Ignore venv, artifacts, logs, etc.
â””â”€â”€ README.md                      # High-level project explanation